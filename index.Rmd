---
title: "Practical Machine Learning"
author: "Vishal Arora"
date: "December 22, 2015"
output: html_document
---

                                                ***Executive Summary***
                                            
In this analysis, we are trying to predict the class of Activity performed by analysing the data generated by
different sensors attached to the body of subject or equipment. Data for this analysis is provided by <http://groupware.les.inf.puc-rio.br/har>. Here we have 159 input parameters and class as the output. I have tried to look for suitable algorithms based on bagging, boosting etc. Boosting algorithms certainly have more Accuracy, however, due to their computation cost and tend to overfit in case of larges Bayes error I have decided to favour Random forest, This algorithm provides better accuracy and less variance as compared to bagging. With Random forest, we were able to achieve .999 accuracy when predicted the test data with confidence Interval from (0.9974, 0.9997). Thus our out of sample Error rate is .1%.

**Data loading and Cleaning**

Data is loaded from csv file provided at the Coursera. In this dataset, quite a few entries are either marked as missing or NA. I have checked for columns where more than 95% of data is either filled with NAs or empty strings. This check has given list of 100 columns. Since columns with such a huge number of NAs couldn't be significant  predictor, I have removed these columns from the dataset. After removing the NA cols, I have looked for columns where there is Zero and near zero variations, As columns with near zero/zero variations does not play significant role in predictions. Above check filtered out one more column. Following this, I decided to drop columns describing name of person, serial number of record. Since performance of activities are not time bound, I have dropped the time columns as well.  Furthermore, output column's data type has been changed to Factor from character, this will help to properly classify the predictions.

```{r, echo = FALSE, results='hide',warning=FALSE }
library(caret)
pmltr <- read.csv("C:\\Arora\\Rprogramming\\Predictive\\CourseProject\\pml-training.csv",stringsAsFactors = FALSE, na.strings = c("NA", "#DIV/0!", ""))
dim(pmltr)
str(pmltr) 											 # Found lots of NA columns
nacount <- apply(pmltr,2,function(x){sum(is.na(x))}) # Found NA count per column
naCols <- unique(which((nacount/dim(pmltr)[1]) > .95)) # Found columns where Number of NA values Occupy more than 95% of the rows
cleantr <- pmltr[,-naCols]	# removing columns with maximum NA Values.
sum(is.na(cleantr))			# It has provided Zero number of NAs.
nearZeroVar(cleantr, saveMetrics=TRUE)		# it checks for columns with No variance and near Zero variance(NZV)
											#	From this we found new_window column to have NZV.
cleantr <- cleantr[,-c(1:6)]	

cleantr$classe <- factor(cleantr$classe)	# converting Output parameter to factor

			# Removing rownumber, name, time of activity as readings are independent of time and subject.

```

**Dividing data into Training and Test subsets**

Following some cleaning of data, I have divided data in to 2 subsets, where 80% of data has gone into Training and 20% in to test. We do not have any separate dataset for testing. Thus, testing subset will provide us good feedback about accuracy of our model derived from training subset.

```{r, echo = FALSE }
inTrain <- createDataPartition(cleantr$classe,p=.8,list = FALSE)	# Dividing training data into 80% training and 20% testing
train_tr <- cleantr[inTrain,]				# Training Set
test_tr <- cleantr[-inTrain,]				# Test Set

```
**Data normalization and outlier extraction**

In this section, I have used BoxCox method to normalise the training data. More skewed data generates predictions with high error rate. After normalising the training data, using quantiles, I found a few outliers in 4 columns. I have filtered those records where I found outliers. Total 4 rows are deleted from the training subset.

Below is the plot featuring columns with outliers and output. ***Outliers*** are evident in the plots.

```{r, echo = FALSE }

pro_train <- preProcess(train_tr, method="BoxCox")	#Running BoxCox method to normalise and scale data
train_tr <- predict(pro_train, train_tr)			# assigning res
featurePlot( x=train_tr[,c("gyros_dumbbell_x","gyros_dumbbell_z","magnet_dumbbell_y","gyros_forearm_y")],y=train_tr$classe, plot="pairs",)

d <- apply(train_tr[,-54],2,function(x){quantile(x)})	#Observing this has given outliers in columns gyros_dumbbell_z, gyros_dumbbell_x, 
														# magnet_dumbbell_y, gyros_forearm_y
train_trap <- train_tr[-c(train_tr$gyros_dumbbell_x < -2),]			# Removing the outlier
train_trap <- train_trap[-c(train_trap$gyros_dumbbell_z > 2),]		# Removing the outlier
train_trap <- train_trap[-c(train_trap$magnet_dumbbell_y < 800),]	# Removing the outlier
train_trap <- train_trap[-c(train_trap$gyros_forearm_y > 7),]		# Removing the outlier

train_tr <- train_trap												# Assigning back to train_tr 


```

**Feature selection using PCA**

In this section, I have tried to find columns with high degree of correlation(more than 90%). Using all columns, which are highly correlated added insignificant accuracy to model on cost of a large computation code. This process has provided me 7 columns whose correlated columns are present in the dataset. Thus, I decided to drop these 7 columns as well.

```{r, echo = FALSE, cache = TRUE , results = 'hide'  }
M <- abs(cor(train_tr[,-54]))
diag(M) <- 0								# These columns can be deleted as they are highly correlated with other existing columns
related <- which(M >0.9, arr.ind = T)		# 5,9,10,11,20,34,47(more than 90% correlation)
train_tr <- train_tr[,-c(5,9,10,11,20,34,47)]

```

**Training the model and Accuracy**
At this moment, training dataset is in a situation to facilitate generation of reliable and less computational intensive model. I have used "OOB" for resampling and number of repetitions to be 10 given the size of training dataset. Number of parameters to choose at each node has been chosen as 28 after a few trials. These options have generated a model which fits the testing data with 99.99% accuracy.

Please Note we have to normalize the test data in the same way as we did the training.

**Below is the confusion matrix on Train Dataset, which is able to Identify all the records correctly**

```{r, echo = FALSE,cache=TRUE}
tr_control <- trainControl(method = "oob",seeds = 3434,number=10, repeats = 10)
tr_grid <- expand.grid(mtry = 5)
modelfit <- train(classe~., method="rf", trControl=tr_control, tuneGrid = tr_grid, data= train_tr)

confusionMatrix(train_tr$classe, predict(modelfit, train_tr[,-which(names(test_tr)=="classe")]))

```
**Below is the confusion matrix on Test Dataset, which clearly states the accuracy of model**
```{r, echo = FALSE,cache=TRUE}

test_tr <- predict(pro_train, test_tr)
confusionMatrix(test_tr$classe, predict(modelfit, test_tr[,-which(names(test_tr)=="classe")]))

```
**Prediction of test dataset**

Following the Normalization of test dataset, prediction with trained model gives us below outcome.

```{r, echo = FALSE, cache=TRUE}
pmltest <- read.csv("C:\\Arora\\Rprogramming\\Predictive\\CourseProject\\pml-testing.csv",stringsAsFactors = FALSE, na.strings = c("NA", "#DIV/0!", ""))
nacount <- apply(pmltest,2,function(x){sum(is.na(x))})
naCols <- unique(which((nacount/dim(pmltest)[1]) > .95))
cleantest <-  pmltest[,-naCols]
cleantest <- predict(pro_train, cleantest)
print(predict(modelfit, cleantest))

```